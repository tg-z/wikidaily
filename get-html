#!/bin/bash

set -e
set -o nounset
set -o errexit
set -o pipefail
IFS=$'\n\t'

_ME="${0##*/}"
_jq="$(command -v jq)"
_curl="$(command -v curl)"
_tree="$(command -v tree)"
timestamp="$(date -I)"
dest="${2:-_site}"

if [ ! -d "${dest}" ]; then
	mkdir -p "${dest}"
fi

help(){
    cat<<__EOF__
usage: ${_ME} <options>

options:
     -h, --help      print usage
     -i, --index     regenerate index.html file
     -l, --list      list daily wikifiles
     -u, --update    update daily wikifile
__EOF__
}

list() {
    "${_tree}" -C "${dest}"
}

index() {

INDEX="wikidaily.html"

for FILE in "${dest}"/*; do
DETAILS="${DETAILS}\
<details>\
<summary>$(basename "${FILE}" | sed 's/.html//')</summary>\
<div>\
$(cat "${FILE}")\
</div>\
</details>"
done

echo "${DETAILS}" > "${INDEX}"
}

download() {

"${_curl}" "https://en.wikipedia.org/w/api.php?format=json&action=query&generator=random&grnnamespace=0&prop=pageimages|extracts&exintro&grnlimit=6" | "${_jq}" -r '.query.pages | [ map(.) | .[] | {title: .title, pageid: .pageid, extract: .extract, thumbnail: .thumbnail.source} ] | .[] | "<img src=" + .thumbnail + "><br><a href=https://en.wikipedia.org/?curid="+(.pageid|tostring)+">"+ .title+"</a><br>"+ .extract+"<br><br>"' > "${dest}"/"${timestamp}".html
}

case "$*" in
    --help|-h)
        help
        ;;
    --index|-i)
	exec ./index
	;;
    --list|-l)
        list
        ;;
    --update|-u)
        shift
        download
        ;;
    *) help
        ;;

esac
